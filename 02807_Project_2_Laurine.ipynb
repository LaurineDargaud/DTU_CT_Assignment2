{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlsPJUw7wCWZ"
   },
   "source": [
    "# 02807: Project 2\n",
    " \n",
    "## Practical information\n",
    " \n",
    "* This project must be completed in groups of 3 students.\n",
    "    * The group must be registered on the course site on DTU Learn: My Course > Groups\n",
    "    * Groups must be registered anew (even if you already registered for Project 1)\n",
    "* This project must be handed in as a jupyter notebook to the course site on DTU Learn. \n",
    "    * Go to the Course Content > Assignments tab to upload your submission. \n",
    "* This project is due on Monday, November 29, 20:00.\n",
    "\n",
    "## Submission rules\n",
    "\n",
    "* Each group has to hand in *one* notebook (`.ipynb`) with their solutions, including a filled out Contribution table (see below).\n",
    "* Your solution must be written in Python.\n",
    "* For each question you should use the cells provided (\"`# your code goes here`\" and \"*your explanation here*\") for your solution\n",
    "    * It is allowed to add code cells within a question block, but consider if it's really necessary.\n",
    "* You should not remove the problem statements, and you should not modify the structure of the notebook.\n",
    "* Your notebook should be runnable and readable from top to bottom.\n",
    "    * Meaning that your code cells work when run in order (from top to bottom).\n",
    "    * Output of any cell depends only on itself and cells above it.\n",
    "* Your notebook should be submitted after having been run from top to bottom.\n",
    "    * This means outputs are interpretable without necessarily running your cells.\n",
    "    * The simplest way to achieve this is using the jupyter menu item Kernel > Restart & Run All just prior to submission. If any cell fails when you do this, your notebook is not ready for submission.\n",
    "    * Exercise 3 in particular will take time to finish, plan accordingly, that is, make sure you have time to run your notebook from top to bottom.\n",
    "* Failure to comply may make it impossible for us to evaluate your submission properly, which will likely negatively impact the points awarded.\n",
    "\n",
    "## Solution guidelines\n",
    "* Data processing is via Spark for the first three exercises and pandas/SQL in the fourth exercise.\n",
    "* Where naming of dataframes and functions are explicitly stated, these must be used.\n",
    "* Your solutions will be evaluated by correctness, code quality and interpretability of the output. \n",
    "    * You have to write clean, readable and efficient Spark code that will generate sensible execution plans.\n",
    "    * You have to write clean, readable and efficient SQL queries.\n",
    "    * Your tables and visualisations should be meaningful and easy to read. This requires, but is not limited to, including headers, legends and well-written (brief) descriptions for graphs/charts. In this step you've found the data processing solution, so put also some effort into its presentation.\n",
    "\n",
    "## Colaboration policy\n",
    " \n",
    "* It is not allowed to collaborate on the exercises with students outside your group, except for discussing the text of the exercise with teachers and fellow students enrolled on the course in the same semester. \n",
    "* It is not allowed to exchange, hand-over or in any other way communicate solutions or parts of solutions to the exercises. \n",
    "* It is not allowed to use solutions from similar courses, or solutions found elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP30rwxHDQyG"
   },
   "source": [
    "## Contribution table and grading\n",
    "\n",
    "* The total amount of points in the project is 150.\n",
    "* You have to indicate who has solved each part of each exercise in a **contribution table**. \n",
    "* A group member can take credit for solving a part of an exercise only if they have contributed **substantially** to the solution. \n",
    "    * Simple contributions, such as correcting a small bug or double-checking the results of functions, are not sufficient for taking credit for a solution.\n",
    "    * Several group members can take credit for the same solution if they all have contributed substantially to it.\n",
    "* Each group member must contribute **at least 65 points**. \n",
    "    * If no name is provided for an exercise's part, **all group members** are considered contributors to it.\n",
    "* Group members should decide amongst themselves how to collaborate on the project to meet these constraints.  \n",
    "* Scores are individual. The score $\\text{score}(m)$ for a group member $m$ ranges from 0 to 10 and is calculated as follows: \n",
    "\n",
    "  * $\\text{individual-score}(m) = \\frac{\\text{total number of points for the parts correctly solved by }m}{\\text{total number of points for the parts contributed by }m}$\n",
    "\n",
    "  * $\\text{group-score} = \\frac{\\text{total number of points correctly solved by any group member}}{\\text{total number of points in the project}}$\n",
    "\n",
    "  * $\\text{score}(m) =  7.5 \\cdot \\text{individual-score}(m) + 2.5 \\cdot \\text{group-score}$\n",
    "  \n",
    "  \n",
    "* The following is an example of a contributions table:\n",
    "\n",
    "|        | Exercise 1 | Exercise 2 | Exercise 3 | Exercise 4 |\n",
    "|--------|------------|------------|------------|------------|\n",
    "| **Part 1** | John       |    Mary        |     Ann       |   Mary, Ann         |\n",
    "| **Part 2** |     Mary       |    Mary        |   Ann         |    John, Ann        |\n",
    "| **Part 3** |     John, Mary, Ann       |      John, Ann      |   John         | John      |\n",
    "| **Part 4** | Ann       |  Ann          |     John, Mary       | John       |\n",
    "| **Part 5** | **n.a.**     | John, Mary, Ann           | **n.a.**       | **n.a.**       |\n",
    "\n",
    "\n",
    "* **Example**: in the contribution table above, suppose that all parts are solved correctly except for those of Exercise 4 which are all wrong. Then Ann's score is calculated as follows:\n",
    "\n",
    "  * $\\text{individual-score}(Ann) = \\frac{5+5+10+5+5+15+15}{5+5+10+5+5+15+15+15+5} = \\frac{60}{80} = 0.75$\n",
    "\n",
    "  * $\\text{group-score} = \\frac{95}{150} = 0.633$\n",
    "\n",
    "  * $\\text{score}(Ann) = 7.5\\cdot 0.75 + 2.5 \\cdot 0.633 = 7.21$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vklet8XdRGVV"
   },
   "source": [
    "# Group contribution table \n",
    "\n",
    "This table must be filled before submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:49.385821Z",
     "start_time": "2021-11-02T13:07:48.995596Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "chiXA3CzRSA1",
    "outputId": "752ba847-32fb-48e7-a4ea-2ce363e3a706"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {'Exercise 1' : ['', '', '', '', 'n.a'], \n",
    "     'Exercise 2' : ['', '', '', '', ''],\n",
    "     'Exercise 3' : ['', '', '', '', 'n.a'],\n",
    "     'Exercise 4' : ['', '', '', '', 'n.a'],\n",
    "     } \n",
    "  \n",
    "ct = pd.DataFrame(d, index=['Part 1', 'Part 2', 'Part 3', 'Part 4', 'Part 5']) \n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPlNQTICo9sH"
   },
   "source": [
    "# The AirBnB dataset\n",
    "\n",
    "<img src=\"https://www.esquireme.com/public/images/2019/11/03/airbnb-678x381.jpg\" alt=\"airbnb\" width=\"400\"/>\n",
    "\n",
    "[Airbnb](http://airbnb.com) is an online marketplace for arranging or offering lodgings. In the first three exercises you will use Spark to analyze data obtained from the Airbnb website (stricly speaking via data scraped by [insideairbnb](http://insideairbnb.com/get-the-data.html)). The purpose of your analysis is to extract insights about listings as a whole, specifics about London, and sentiment analysis of reviews (word positivity).\n",
    "\n",
    "\n",
    "## Loading data\n",
    "The dataset consists of listings (offered lodgings) and reviews (submitted by users). The `.csv`'s you'll work with vary between the first three exercises, but is structured so that the function below will load it into a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:51.283648Z",
     "start_time": "2021-11-02T13:07:51.280806Z"
    },
    "id": "E9BEj7re4jnN"
   },
   "outputs": [],
   "source": [
    "def load_csv_as_dataframe(path):\n",
    "    return spark.read.option('header', True) \\\n",
    "                .option('inferSchema', True) \\\n",
    "                .option('multiLine', 'True') \\\n",
    "                .option('escape', '\"') \\\n",
    "                .option('mode', 'DROPMALFORMED')\\\n",
    "                .csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4hk-43Io9sI"
   },
   "source": [
    "## Imports and Spark session\n",
    "\n",
    "* You'll need to adapt the `JAVA_HOME` environment variable to your setup. \n",
    "* You should set the `spark.driver.memory` value to the amount of memory on your machine. \n",
    "* It may be required for you to install some of the packages imported below (e.g. pandasql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:32:31.290655Z",
     "start_time": "2021-11-02T13:32:31.284171Z"
    },
    "id": "Zy1rMfRh4jnN"
   },
   "outputs": [],
   "source": [
    "# Instructions on p. 20 Learning Spark, 2nd ed.\n",
    "# Here's a quick-guide, googling may also be required\n",
    "# 1) Install pyspark via conda/pip\n",
    "#          pyspark requires the JAVA_HOME environment variable is set.\n",
    "# 2) Install JDK 8 or 11, figure out the install location\n",
    "#          Suggest to use https://adoptopenjdk.net/\n",
    "# 3) Update the JAVA_HOME environment variable set programmatically below \n",
    "#    with your install location specifics\n",
    "\n",
    "# JAVA_HOME environment variable is set programatically below\n",
    "# but you must point it to your local install\n",
    "\n",
    "import os\n",
    "#os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/'\n",
    "# my Java path\n",
    "os.environ[\"JAVA_HOME\"] = \"C://Program Files//Eclipse Adoptium//jdk-11.0.13.8-hotspot\"\n",
    "\n",
    "# If you get \"Job aborted due to stage failure\" and \n",
    "# \"Python worker failed to connect back.\" exceptions, \n",
    "# this should be solved by additionally setting these \n",
    "# environment variables\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:53.807655Z",
     "start_time": "2021-11-02T13:07:52.901610Z"
    },
    "id": "2ftW7yaGo9sJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:57.883383Z",
     "start_time": "2021-11-02T13:07:53.808476Z"
    },
    "id": "OBILY9sf4jnN"
   },
   "outputs": [],
   "source": [
    "# Sets memory limit on driver and to use all CPU cores\n",
    "conf = SparkConf().set('spark.ui.port', '4050') \\\n",
    "        .set('spark.driver.memory', '4g') \\\n",
    "        .setMaster('local[*]')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:58.627425Z",
     "start_time": "2021-11-02T13:07:57.884329Z"
    },
    "id": "cJpOoxzN4jnN",
    "outputId": "517231b3-51f5-4e7f-9481-8384d0989a00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.209.225.166:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2409e65c610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:58.667031Z",
     "start_time": "2021-11-02T13:07:58.628174Z"
    },
    "id": "L5lJlshk4jnN",
    "outputId": "95c2c9ea-e61f-4ac1-ab01-14b494a549c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '52071'),\n",
       " ('spark.app.id', 'local-1638105492878'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/C:/Users/Laurine/Documents/DTU%20Python/Computational%20Tools%20for%20Data%20Science/Assignment%202/spark-warehouse'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.driver.host', '10.209.225.166'),\n",
       " ('spark.ui.port', '4050'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.startTime', '1638105491305'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PdveMrU4jnO"
   },
   "source": [
    "# Exercise 1: Listings and cities (20 pts)\n",
    "\n",
    "In this exercise you must use Spark to do the data processing. \n",
    "* For parts where you present tabular data, this entails calling `toPandas` as the final step of your query. \n",
    "* For parts requiring visualisation, the `toPandas` call should be followed only by functions necessary to customize the plotting/layout steps (i.e. no data processing take place after your spark dataframe is materialized).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5LAuprs4jnO"
   },
   "source": [
    "## Part 1: Preparing the dataframe (5 pts)\n",
    "\n",
    "Your data source is [this zip archive](https://data-download.compute.dtu.dk/c02807/listings.csv.zip) which you must uncompress and place in the same folder as this notebook. It is loaded in the next cell and named `df_listings`.\n",
    "\n",
    "After the data is read, you should select the columns necessary for exercise 1, 2 and 3 (by reading ahead or iteratively extend this loading code). Name this dataframe `df_listings_analysis` and make use of caching.\n",
    "\n",
    "Prices are in local currency, but are nonetheless prefixed with `$` and contains thousands separator commas. You will need to remove these characters and cast the price column to `pyspark.sql.types.DoubleType`. Observe that if this casting is not possible, the result of the cast is `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:29:07.681609Z",
     "start_time": "2021-10-27T15:28:34.756446Z"
    },
    "id": "NTybzL654jnO"
   },
   "outputs": [],
   "source": [
    "df_listings = load_csv_as_dataframe('listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:35:36.175515Z",
     "start_time": "2021-10-31T14:35:36.170621Z"
    },
    "id": "qnbDRPXH4jnO"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "df_listings_analysis = df_listings.select(F.col('city'),\\\n",
    "                                          F.col('neighbourhood_cleansed'),\\\n",
    "                                          F.col('property_type'), \\\n",
    "                                          F.col('review_scores_rating').cast(pyspark.sql.types.DoubleType()),\n",
    "                                          F.regexp_replace(F.col('price'),\"[\\$,]\",\"\").alias('price').cast(pyspark.sql.types.DoubleType()),\\\n",
    "                                          F.col('id') \\\n",
    "                                         ).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QZ0w17Y4jnO"
   },
   "source": [
    "## Part 2: Listing and neighbourhood counts (5 pts)\n",
    "\n",
    "Compute and visualise the number of listings and the number of different neighbourhoods per city, restricted to the 15 cities having the most listings. The x-axis should be ordered by number of listings (high to low).\n",
    "\n",
    "Make sure to use the `neighbourhood_cleansed` column in your computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:35:44.882818Z",
     "start_time": "2021-10-31T14:35:44.880268Z"
    },
    "id": "kxo1ZH_q4jnO"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "# get the count of listings for the 15 most listed cities\n",
    "nb_listings_per_city = df_listings_analysis \\\n",
    "    .select(F.col('city').alias('city2')) \\\n",
    "    .groupby('city2') \\\n",
    "    .count() \\\n",
    "    .sort(F.desc('count')) \\\n",
    "    .limit(15) \\\n",
    "# get the count of neighbourhoods per city \n",
    "nb_neighbourhoods_per_city = df_listings_analysis \\\n",
    "    .groupby('city') \\\n",
    "    .agg(F.countDistinct('neighbourhood_cleansed'))\n",
    "# join tables to get the count of listings and neighbourhoods for the 15 most listed cities\n",
    "joined_nb_listings_per_city = nb_listings_per_city.join(nb_neighbourhoods_per_city, nb_neighbourhoods_per_city['city'] == nb_listings_per_city['city2'], 'left').sort(F.desc('count')).drop('city2')\n",
    "\n",
    "# to pandas\n",
    "pd_15cities = joined_nb_listings_per_city.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.bar(pd_15cities['city'], pd_15cities['count'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Nb of listings per city (15 most listed)\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.bar(pd_15cities['city'], pd_15cities['count(neighbourhood_cleansed)'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Nb of neighbourhoods per city (15 most listed)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9BFleEJ4jnO"
   },
   "source": [
    "## Part 3: Price averages (5 pts)\n",
    "\n",
    "Compute and visualise the average price of listings per city, restricted to the 15 cities having the most listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "# get the average price of listings per city\n",
    "avg_price_per_city = df_listings_analysis.groupby('city').avg('price')\n",
    "# join tables to get the average price of listings per city for the 15 most listed cities\n",
    "joined_avg_price_per_city = nb_listings_per_city.join(avg_price_per_city, avg_price_per_city['city'] == nb_listings_per_city['city2'], 'left').sort(F.desc('count')).drop('city2')\n",
    "\n",
    "# to pandas\n",
    "pd_avgprice = joined_avg_price_per_city.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "plt.bar(pd_avgprice['city'], pd_avgprice['avg(price)'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Average price of listings per city (15 most listed)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIpHBUBk4jnO"
   },
   "source": [
    "## Part 4: Value for money (5 pts)\n",
    "\n",
    "The value of a listing is its rating divided by its price. The value of a city is the average value of its listings. \n",
    "\n",
    "Prices are only comparable when the local currency is the same. We'll therefore consider a subset of Euro-zone cities as defined in `eurozone_cities`.\n",
    "\n",
    "Compute and visualise the value per city, restricted to the Euro-zone cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:29:53.566725Z",
     "start_time": "2021-10-27T15:29:53.565232Z"
    },
    "id": "wqspMVZ24jnO"
   },
   "outputs": [],
   "source": [
    "eurozone_cities = [\n",
    "    'Paris', 'Roma', 'Berlin', 'Madrid', 'Amsterdam', 'Barcelona', 'Milano', 'Lisboa',\n",
    "    'München', 'Wien', 'Lyon', 'Firenze', 'Porto', 'Napoli', 'Bordeaux', 'Venezia',\n",
    "    'Málaga', 'Sevilla', 'València'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:35:57.257011Z",
     "start_time": "2021-10-31T14:35:57.253676Z"
    },
    "id": "YtktiltR4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "# filter on Euro-zone cities\n",
    "filter_eurozone = F.col('city').isin(eurozone_cities)\n",
    "# get the value of a listing\n",
    "listing_value = F.col('review_scores_rating') / F.col('price')\n",
    "\n",
    "value_per_city = df_listings_analysis \\\n",
    "    .filter(filter_eurozone) \\\n",
    "    .withColumn('listing_value',listing_value) \\\n",
    "    .groupby('city') \\\n",
    "    .avg('listing_value') \\\n",
    "    .sort(F.desc('avg(listing_value)'))\n",
    "\n",
    "# to pandas\n",
    "pd_value_per_city = value_per_city.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(pd_value_per_city['city'], pd_value_per_city['avg(listing_value)'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"The value per city in Euro-zone\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbJdSKC84jnP"
   },
   "source": [
    "# Exercise 2: The case of London (30 pts)\n",
    "\n",
    "In this exercise you must use Spark to do the data processing. \n",
    "* For parts where you present tabular data, this entails calling `toPandas` as the final step of your query. \n",
    "* For parts requiring visualisation, the `toPandas` call should be followed only by functions necessary to customize the plotting/layout steps (i.e. no data processing take place after your spark dataframe is materialized). \n",
    "* You may need multiple queries to solve the individual parts.\n",
    "\n",
    "Your dataframe is a subset of `df_listings_analysis` and should be named `df_listings_london`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:05.783859Z",
     "start_time": "2021-10-31T14:36:05.781830Z"
    },
    "id": "aeVpGvZF4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "df_listings_london = df_listings_analysis.filter(F.col('city')=='London')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-KJqz1H4jnP"
   },
   "source": [
    "## Part 1: Price distribution (5 pts)\n",
    "\n",
    "Compute and visualise the distribution of prices, for all prices up to and including the 95-percentile. Additionally, compute and visualise the distribution of prices, for all prices above the 95-percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:15.616156Z",
     "start_time": "2021-10-31T14:36:15.613196Z"
    },
    "id": "kWc-mqph4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "# get the value of the 95-percentile\n",
    "value_95percentile = df_listings_london.agg(F.percentile_approx('price', [0.95]).alias('95-percentile')).collect()[0]['95-percentile'][0]\n",
    "\n",
    "# compute prices below then above the 95-percentile value\n",
    "prices_below_95percentile = df_listings_london.filter(F.col('price') <= value_95percentile).select('price')\n",
    "prices_above_95percentile = df_listings_london.filter(F.col('price') > value_95percentile).select('price')\n",
    "\n",
    "# to pandas\n",
    "pd_prices_below_95percentile = prices_below_95percentile.toPandas()\n",
    "pd_prices_above_95percentile = prices_above_95percentile.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_prices_below_95percentile.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_prices_above_95percentile.hist(bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2Oa37Ck4jnP"
   },
   "source": [
    "## Part 2: Prices by type of property (5 pts)\n",
    "\n",
    "Compute and visualise the average price and average rating per type of property, for property types with 75 or more listings. \n",
    "\n",
    "Your visualisation should be a single bar chart with two y-axes and two bars per property type. The x-axis should be ordered by average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:21.187516Z",
     "start_time": "2021-10-31T14:36:21.179099Z"
    },
    "id": "b5jx-W9O4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "filter_property = F.col('count')>= 75\n",
    "filtered_property_types = df_listings_london \\\n",
    "    .select(F.col('property_type').alias('property_type2')) \\\n",
    "    .groupby('property_type2') \\\n",
    "    .count() \\\n",
    "    .sort(F.desc('count')) \\\n",
    "    .filter(filter_property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price_per_property_type = df_listings_london.groupby('property_type').avg('price')\n",
    "avg_rating_per_property_type = df_listings_london.groupby('property_type').avg('review_scores_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_filtered_property_types = filtered_property_types.join(avg_price_per_property_type,avg_price_per_property_type['property_type']==filtered_property_types['property_type2'] ).drop('property_type')\n",
    "joined_filtered_property_types = joined_filtered_property_types.join(avg_rating_per_property_type,avg_rating_per_property_type['property_type']==filtered_property_types['property_type2']).drop('property_type2')\n",
    "joined_filtered_property_types = joined_filtered_property_types.sort(F.desc('avg(review_scores_rating)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_per_property_type = joined_filtered_property_types.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : we want 2 y-axis with different scale ??\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "x_bars, width = np.arange(len(pd_per_property_type.property_type)), 0.35\n",
    "\n",
    "plt.bar(x_bars-width/2, pd_per_property_type['avg(review_scores_rating)'], width, label='avg(review_scores_rating)')\n",
    "plt.tick_params(axis='y', labelleft='off', labelright='on')\n",
    "\n",
    "plt.bar(x_bars+width/2, pd_per_property_type['avg(price)'], width, label='avg(price)')\n",
    "plt.tick_params(axis='y', labelleft='on', labelright='off')\n",
    "\n",
    "plt.title(\"Average price and average rating per type of property, for property types with 75 or more listings\")\n",
    "plt.xticks(ticks=x_bars,rotation=90, labels=pd_per_property_type.property_type)\n",
    "plt.xlabel(\"Property type\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvUyAWA94jnP"
   },
   "source": [
    "## Part 3: Best offering in the neighbourhood (10 pts)\n",
    "\n",
    "The value of a listing is its rating divided by its price. Compute and display a dataframe (with the columns you selected in Exercise 1 and those computed in this part) with the 3 highest valued listings in each neighbourhood, and having a value above 5. Make sure to use the `neighbourhood_cleansed` column in your computations.\n",
    "\n",
    "Computing ranks based on value can be achieved using `pyspark.sql.window.Window`. This may produce equal ranks (i.e. when the value of two listings are the same).\n",
    "\n",
    "Remember to use `pd.set_option('display.max_rows', <n>)` with appropriate `<n>` so all rows are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:05.393213Z",
     "start_time": "2021-10-27T15:30:03.259525Z"
    },
    "id": "8lmAGdHw4jnQ"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# get the value of a listing\n",
    "listing_value = F.col('review_scores_rating') / F.col('price')\n",
    "filter_value_5 = F.col('listing_value') > 5\n",
    "\n",
    "value_rank_in_neighborhood = df_listings_london \\\n",
    "    .withColumn('listing_value',listing_value) \\\n",
    "    .filter(filter_value_5)\n",
    "\n",
    "# window: 3 highest values listings in each neighbourhood having a value above 5\n",
    "value_window = Window \\\n",
    "    .partitionBy(\"neighbourhood_cleansed\") \\\n",
    "    .orderBy(value_rank_in_neighborhood[\"listing_value\"].desc())\n",
    "\n",
    "filter_rank_3 = F.col('row_number') <= 3\n",
    "\n",
    "value_rank_in_neighborhood = value_rank_in_neighborhood \\\n",
    "    .withColumn('row_number',row_number().over(value_window)) \\\n",
    "    .filter(filter_rank_3) \\\n",
    "    .sort(F.col('neighbourhood_cleansed'), F.col('row_number'))\n",
    "\n",
    "# to pandas\n",
    "pd_value_rank_in_neighborhood = value_rank_in_neighborhood.toPandas()\n",
    "\n",
    "# show pandas\n",
    "pd.set_option('display.max_rows', len(pd_value_rank_in_neighborhood))\n",
    "pd_value_rank_in_neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZdZ5-FE4jnQ"
   },
   "source": [
    "## Part 4: Activity by month (5 pts)\n",
    "\n",
    "Activity is given by the number of reviews received in a given time period. Compute and visualise the activity based on month, that is, the total number of reviews given in January, February, etc..\n",
    "\n",
    "Your additional data source is [this zip archive](https://data-download.compute.dtu.dk/c02807/reviews_london.csv.zip) which you must uncompress and place in the same folder as this notebook. It is loaded in the next cell and named `df_reviews_london`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:07.720386Z",
     "start_time": "2021-10-27T15:30:05.404386Z"
    },
    "id": "MGtA6uzq4jnQ"
   },
   "outputs": [],
   "source": [
    "df_reviews_london = load_csv_as_dataframe('reviews_london.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:42.395920Z",
     "start_time": "2021-10-31T14:36:42.393979Z"
    },
    "id": "NohJUbEg4jnQ"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "df_reviews_london = df_reviews_london.withColumn('month',F.month(F.to_date(F.col('date'))))\n",
    "df_count_reviews_per_month = df_reviews_london.groupby('month').count().sort('month')\n",
    "\n",
    "# to pandas\n",
    "pd_df_count_reviews_per_month = df_count_reviews_per_month.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "import datetime\n",
    "\n",
    "pd_df_count_reviews_per_month.plot.bar(x='month', y='count', width=0.7)\n",
    "plt.title('Activity by month')\n",
    "plt.xticks(ticks=list(range(12)), rotation=0, labels=[datetime.datetime.strptime(str(month_num), \"%m\").strftime(\"%b\") for month_num in pd_df_count_reviews_per_month['month']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So8nbHnR4jnQ"
   },
   "source": [
    "## Part 5: Reviews per listing (5 pts)\n",
    "\n",
    "Each London listing has received 0 or more reviews. \n",
    "\n",
    "Display a dataframe showing 1) The number of listings, 2) The average number of reviews a listing receives, 3) The standard deviation of the reviews per listing distribution, 4) The minimum number of reviews any listing has received, and 5) The maximum number of reviews any listing has received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:48.293254Z",
     "start_time": "2021-10-31T14:36:48.291307Z"
    },
    "id": "xnH01VPh4jnQ"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "join_review_listing = df_listings_london.join(df_reviews_london, df_reviews_london['listing_id']==df_listings_london['id'])\n",
    "\n",
    "review_listing_summary = join_review_listing.groupby('listing_id').count().describe().select(\n",
    "    F.col('summary'),\n",
    "    F.col('count').alias('reviews_per_listing'))\n",
    "\n",
    "# to pandas\n",
    "pd_review_listing_summary = review_listing_summary.toPandas().T\n",
    "pd_review_listing_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x62poLc34jnQ"
   },
   "source": [
    "# Exercise 3: Word sentiment (45 pts)\n",
    "\n",
    "In this exercise you must use Spark to do the data processing. For parts where you present tabular data, this entails calling `toPandas` as the final step of your query. You may need multiple queries to solve the individual parts.\n",
    "\n",
    "The goal here is to determine what sentiment (positive or negative) words in reviews have. Roughly speaking, we want each word to be assigned a score based on the rating of the reviews in which the word occurs in the review comment. We'd expect words such as \"clean\", \"comfortable\", \"superhost\" to receive high scores, while words such as \"unpleasant\", \"dirty\", \"disgusting\" would receive low scores.\n",
    "\n",
    "As individual reviews do not have a rating, we'll consider the rating of individual reviews to be the rating of its related listing (i.e. assuming each review gave the average rating (`review_scores_rating`) of the listing). \n",
    "\n",
    "The score of a word is given by the mean review rating over the reviews in which that word occurs in the comment. We require words to appear in at least 0.5% (1 in 200) listings, and to be at least 4 characters, for it to have a defined score.\n",
    "\n",
    "Formally, when a word $w$ occurs in at least $0.5\\%$ of listings and $|w| > 3$, its score is\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "score(w) = \\frac{1}{|C_w|}\\sum_{comment \\in C_w} \\text{review_rating}(comment)\n",
    "\\end{align*}\n",
    "$\n",
    ", where \n",
    "* $C_w = \\{comment \\mid w \\text{ occurs in } \\text{clean_text}(comment)\\}$, the set (so no duplicates) of comments in which $w$ occurs, and\n",
    "* $\\text{clean_text}(comment)$ is the result of your `clean_text` function defined below, and\n",
    "* $\\text{review_rating}(comment)$ is the `review_scores_rating` of the listing which this $comment$ is related to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFNxM4PS4jnQ"
   },
   "source": [
    "## Part 1: Toy data (15 pts)\n",
    "\n",
    "To get started we'll consider a toy example where the input is `df_sentiment_listings_toy` and `df_sentiment_reviews_toy` defined in the next code cell. You should provide an implementation of `calculate_word_scores_toy` in the subsequent code cell. Your implementation should result in a query that when given the toy example dataframes as input and is materialized with `toPandas()` produces this table:\n",
    "\n",
    "|    | word   |   word_score |   listing_occurences |   word_occurences |   comment_occurences |\n",
    "|---:|:-------|-------------:|---------------------:|------------------:|---------------------:|\n",
    "|  0 | aaaa   |      7       |                    3 |                 5 |                    5 |\n",
    "|  1 | bbbb   |      6.66667 |                    2 |                 3 |                    3 |\n",
    "|  2 | eeee   |      0       |                    1 |                 1 |                    1 |\n",
    "|  3 | dddd   |      5       |                    1 |                 1 |                    1 |\n",
    "|  4 | cccc   |      5       |                    2 |                 2 |                    2 |'\n",
    "\n",
    "Observe that `word_occurences` and `comment_occurences` are the same as words occuring multiple times in a comment are counted once, and that `clean_text` is used to ignore casing and discard non-words. Additionally, any word occuring at least once will occur in more than 1 out of 200 listings on this toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:20.311738Z",
     "start_time": "2021-10-27T15:30:20.131208Z"
    },
    "id": "p2_01YSQ4jnQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema_listings = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('review_scores_rating', StringType(), True),\n",
    "])\n",
    "data_listings = [\n",
    "    {'id': '0', 'review_scores_rating': '10'},\n",
    "    {'id': '1', 'review_scores_rating': '5'},\n",
    "    {'id': '2', 'review_scores_rating': '0'},\n",
    "]\n",
    "df_sentiment_listings_toy = spark.createDataFrame(data_listings, schema_listings)\n",
    "\n",
    "schema_reviews = StructType([\n",
    "    StructField('listing_id', StringType(), True),\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('comments', StringType(), True),\n",
    "])\n",
    "data_reviews = [\n",
    "    {'listing_id': '0', 'id': '100', 'comments': 'aaaa bbbb          cccc'},\n",
    "    {'listing_id': '0', 'id': '101', 'comments': 'aaaa bbbb '},\n",
    "    {'listing_id': '0', 'id': '102', 'comments': 'aaaa aAAa          aaaa'},\n",
    "    {'listing_id': '1', 'id': '103', 'comments': 'Aaaa bbb ccc'},\n",
    "    {'listing_id': '1', 'id': '104', 'comments': 'dddd %ˆ&*'},\n",
    "    {'listing_id': '2', 'id': '105', 'comments': 'AaaA'},\n",
    "    {'listing_id': '2', 'id': '106', 'comments': 'bbbb ccc e&eˆˆee'},\n",
    "    {'listing_id': '2', 'id': '107', 'comments': 'cccc cccc'},\n",
    "]\n",
    "\n",
    "df_sentiment_reviews_toy = \\\n",
    "    spark.createDataFrame(data_reviews, schema_reviews) \\\n",
    "        .select(F.col('listing_id'), F.col('id').alias('comment_id'), F.col('comments'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:24.249616Z",
     "start_time": "2021-10-27T15:30:20.312564Z"
    },
    "id": "ybRkPjCD4jnR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_score</th>\n",
       "      <th>listing_occurences</th>\n",
       "      <th>word_occurences</th>\n",
       "      <th>comment_occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaa</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bbbb</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eeee</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dddd</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cccc</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  word_score  listing_occurences  word_occurences  comment_occurences\n",
       "0  aaaa    7.000000                   3                5                   5\n",
       "1  bbbb    6.666667                   2                3                   3\n",
       "2  eeee    0.000000                   1                1                   1\n",
       "3  dddd    5.000000                   1                1                   1\n",
       "4  cccc    5.000000                   2                2                   2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(col):\n",
    "    \"\"\"\n",
    "        Cleans the text (comment) associated with col. The\n",
    "        cleaning should:\n",
    "            1) Lower case the text\n",
    "            2) Turn multiple whitespaces into single whitespaces\n",
    "            3) Remove anything but letters, digits and whitespaces\n",
    "        \n",
    "        :col: A Spark Column object containing text data\n",
    "        :returns: A Spark Column object.\n",
    "    \"\"\"\n",
    "    # 1) Lower case the text\n",
    "    col = col.lower()\n",
    "    # 2) Turn multiple whitespaces into single whitespaces\n",
    "    col = re.sub(\"\\s+\",' ', col)\n",
    "    # 3) Remove anything but letters, digits and whitespaces\n",
    "    col = re.sub('[^a-z\\d\\s]+','',col)\n",
    "    \n",
    "    return col\n",
    "\n",
    "def calculate_word_scores_toy(df_list, df_rev):\n",
    "    \"\"\"\n",
    "        Calculates the word score over listings in df_list and\n",
    "        reviews in df_rev. The table produced should have the \n",
    "        same columns as specified in part 1.\n",
    "        \n",
    "        :returns: A pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # apply clean_text on reviews' comments\n",
    "    clean_comment = F.udf(clean_text)\n",
    "    df_clean_comments = df_rev.withColumn('clean_comments', clean_comment(F.col('comments')))\n",
    "    # filter only words with more than 3 characters + explode\n",
    "    filter_len_word = F.length('word') > 3\n",
    "    df_saying = df_clean_comments.withColumn('word', F.explode(F.split(F.col('clean_comments'), ' '))).filter(filter_len_word)\n",
    "    # join with listings \n",
    "    df_joined = df_saying.join(df_list, df_saying['listing_id']==df_list['id'], 'left')\n",
    "    # drop duplicates\n",
    "    df_joined = df_joined.dropDuplicates()\n",
    "    # filter on listing_occurences\n",
    "    nb_listings = df_list.count()\n",
    "    filter_listings = F.col('listing_occurences') > nb_listings/200\n",
    "    # get occurences + filter\n",
    "    df_occurences = df_joined.groupby('word').agg(\n",
    "        F.mean('review_scores_rating').alias('word_score'),\n",
    "        F.countDistinct('listing_id').alias('listing_occurences'),\n",
    "        F.count('word').alias('word_occurences'),\n",
    "        F.countDistinct('comment_id').alias('comment_occurences')\n",
    "    ).filter(filter_listings)\n",
    "    \n",
    "    return df_occurences.toPandas()\n",
    "    \n",
    "calculate_word_scores_toy(df_sentiment_listings_toy, df_sentiment_reviews_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF0-Et6Y4jnR"
   },
   "source": [
    "## Part 2: London comments (15 pts)\n",
    "\n",
    "In this part we'll calculate word scores for the comments related to London listings only. You should implement `count_relevant_listings` and `calculate_word_scores` (it will be an extension of your function from part 1) below. See the mathematical definition and docstrings for intended behaviour.\n",
    "\n",
    "The function `calculate_word_scores` should return the top 10 and bottom 10 words by score. You should **not** use caching in your function.\n",
    "\n",
    "Make sure your satisfy all conditions for a word to be scored (e.g. correctly calculating how many total listings scores are computed over). You should also consider whether your query is optimally structured in terms of computation time. Moreover, `pd.set_option('display.max_rows', <n>)` should be set with sufficiently high `n` to show all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T13:35:09.413747Z",
     "start_time": "2021-10-31T13:35:09.386493Z"
    },
    "id": "NL05dnDH4jnR"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "def count_relevant_listings(df_list, df_rev):\n",
    "    \"\"\"\n",
    "        Calculates the number of listings in df_list that has a \n",
    "        review in df_rev. A listing that is reviewed more than once\n",
    "        should only count as one.\n",
    "        \n",
    "        :returns: An integer \n",
    "    \"\"\"\n",
    "    \n",
    "    df_joined = df_list.join(df_rev, df_list['id']==df_rev['listing_id'], how='inner')\n",
    "    \n",
    "    return df_joined.select('listing_id').distinct().count()\n",
    "    \n",
    "\n",
    "def calculate_word_scores(df_list, df_rev, listings_count):\n",
    "    \"\"\"\n",
    "        Calculates the word score over listings in df_list and\n",
    "        reviews in df_rev. The value of listings_count should \n",
    "        be used to filter out words not occuring frequently enough\n",
    "        in comments. The table produced should have the same columns\n",
    "        as in part 1 of this exercise.\n",
    "        \n",
    "        :returns: A pandas DataFrame containing the top 10 and \n",
    "        bottom 10 words based on their word score, sorted by word_score.\n",
    "    \"\"\"\n",
    "    # select useful columns\n",
    "    df_rev = df_rev.select('listing_id','id','comments').withColumnRenamed('id','comment_id')\n",
    "    df_list = df_list.select('id','review_scores_rating')\n",
    "    \n",
    "    # Modifing df_rev\n",
    "    clean_comment = F.udf(clean_text)\n",
    "    \n",
    "    # !! remove rows with no comment (null values)\n",
    "    df_rev = df_rev.filter(F.col('comments').isNotNull())\n",
    "    \n",
    "    df_list = df_list.filter(F.col('review_scores_rating').isNotNull())\n",
    "    #  apply clean_text on reviews' comments\n",
    "    clean_comment = F.udf(clean_text)\n",
    "    \n",
    "    df_rev = df_rev.withColumn('clean_comments', clean_comment(F.col('comments'))).drop('comments')\n",
    "    # explode to count words\n",
    "    df_rev = df_rev.withColumn('word', F.explode(F.split(F.col('clean_comments'), ' ')))\n",
    "    \n",
    "    #!! Keep only relevant columns\n",
    "    df_rev=df_rev.select('listing_id','comment_id','word')\n",
    "    \n",
    "    #   join with listings \n",
    "    df_joined = df_list.join(df_rev, df_rev['listing_id']==df_list['id'], 'inner')\n",
    "\n",
    "    # filter on listing_occurences\n",
    "    filter_listings = F.col('listing_occurences') > listings_count/200\n",
    "    # get occurences + filter\n",
    "    df_occurences = df_joined.groupby('word').agg(\n",
    "        F.mean('review_scores_rating').alias('word_score'),\n",
    "        F.countDistinct('listing_id').alias('listing_occurences'),\n",
    "        F.count('word').alias('word_occurences'),\n",
    "        F.countDistinct('comment_id').alias('comment_occurences')\n",
    "    ).filter(filter_listings)\n",
    "\n",
    "     # get the top 10 words\n",
    "    df_sorted = df_occurences.sort(F.desc('word_score')).withColumn(\"index\", monotonically_increasing_id())\n",
    "    \n",
    "    df_top10= df_sorted.filter(F.col('index')<10)\n",
    "    df_bottom10=df_sorted.filter(F.col('index')>df_top10.count()-1-10)\n",
    "    # get the bottom 10 words\n",
    "#     df_bottom10 = df_occurences.sort(F.asc('word_score')).limit(10)\n",
    "    # concatenate top10 and bottom10\n",
    "    df_final = df_top10.union(df_bottom10).select('word','word_score','listing_occurences','word_occurences','comment_occurences')\n",
    "    \n",
    "    return df_final.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T13:37:21.918401Z",
     "start_time": "2021-10-31T13:35:10.034363Z"
    },
    "id": "ZoUPZ6F-4jnR",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_score</th>\n",
       "      <th>listing_occurences</th>\n",
       "      <th>word_occurences</th>\n",
       "      <th>comment_occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>granola</td>\n",
       "      <td>97.562753</td>\n",
       "      <td>153</td>\n",
       "      <td>247</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>homemade</td>\n",
       "      <td>97.283971</td>\n",
       "      <td>474</td>\n",
       "      <td>1229</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yoghurt</td>\n",
       "      <td>97.041284</td>\n",
       "      <td>146</td>\n",
       "      <td>218</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yogurt</td>\n",
       "      <td>97.033079</td>\n",
       "      <td>245</td>\n",
       "      <td>393</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>superhost</td>\n",
       "      <td>97.005917</td>\n",
       "      <td>687</td>\n",
       "      <td>845</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7755</th>\n",
       "      <td>hostel</td>\n",
       "      <td>85.449848</td>\n",
       "      <td>677</td>\n",
       "      <td>1974</td>\n",
       "      <td>1723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7756</th>\n",
       "      <td>worst</td>\n",
       "      <td>85.294430</td>\n",
       "      <td>641</td>\n",
       "      <td>754</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7757</th>\n",
       "      <td>sucio</td>\n",
       "      <td>84.545455</td>\n",
       "      <td>153</td>\n",
       "      <td>176</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7758</th>\n",
       "      <td>disgusting</td>\n",
       "      <td>84.534296</td>\n",
       "      <td>245</td>\n",
       "      <td>277</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7759</th>\n",
       "      <td>filthy</td>\n",
       "      <td>82.821608</td>\n",
       "      <td>328</td>\n",
       "      <td>398</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7760 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  word_score  listing_occurences  word_occurences  \\\n",
       "0        granola   97.562753                 153              247   \n",
       "1       homemade   97.283971                 474             1229   \n",
       "2        yoghurt   97.041284                 146              218   \n",
       "3         yogurt   97.033079                 245              393   \n",
       "4      superhost   97.005917                 687              845   \n",
       "...          ...         ...                 ...              ...   \n",
       "7755      hostel   85.449848                 677             1974   \n",
       "7756       worst   85.294430                 641              754   \n",
       "7757       sucio   84.545455                 153              176   \n",
       "7758  disgusting   84.534296                 245              277   \n",
       "7759      filthy   82.821608                 328              398   \n",
       "\n",
       "      comment_occurences  \n",
       "0                    244  \n",
       "1                   1200  \n",
       "2                    216  \n",
       "3                    387  \n",
       "4                    822  \n",
       "...                  ...  \n",
       "7755                1723  \n",
       "7756                 707  \n",
       "7757                 163  \n",
       "7758                 258  \n",
       "7759                 345  \n",
       "\n",
       "[7760 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5min 29s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "329.8383942999999"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should not be modified\n",
    "from IPython.display import display\n",
    "\n",
    "relevant_listings_count_london = count_relevant_listings(df_listings_london, df_reviews_london)\n",
    "word_scores_london_timing = %timeit -o -n1 -r1 display( \\\n",
    "    calculate_word_scores(df_listings_london, \\\n",
    "                          df_reviews_london, \\\n",
    "                          relevant_listings_count_london) \\\n",
    ")\n",
    "\n",
    "word_scores_london_timing.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAPvcNbU4jnR"
   },
   "source": [
    "## Part 3: Scalability (10 pts)\n",
    "\n",
    "The listings from London make up a little less than 2% of the entire set of listings. In this part we're interested in how the amount of input data impacts computation time, that is, how `calculate_word_scores` scales as data increases. To this end, we've made multiple samples of the dataset of varying sizes.\n",
    "\n",
    "The experiment reuses `count_relevant_listings` and `calculate_word_scores` that you implemented in part 2. Code needed for this part is provided to you. \n",
    "\n",
    "Your task is to obtain the data sources, run the code cells below, and explain the results you get. Specifically, you must explain any non-linear relationship between data size and computation time, using the markdown cell at the end of this part. In finding explanations, using the Spark UI to investigate the anatomy of your queries may prove valuable. Once you've found an explanation, state a potential solution to remedy the issue. Lastly, include a paragraph stating the specifications of your computer hardware (memory, CPU cores and clock speed, solid state disk or not) on which the experiment has been run.\n",
    "\n",
    "*Implementation note* Make sure you've properly configured `spark.driver.memory` (it requires a kernel restart to update the value). It may be that your query fails on the larger samples due to running out of compute resources. This is likely caused by a suboptimal `calculate_word_scores`, but can be from reaching the limits of your hardware. If you think the latter is the case, argue for this perspective in the markdown cell.\n",
    "\n",
    "Your data sources are (uncompress and place in the same directory as this notebook):\n",
    "* 0.25%: [listings](https://data-download.compute.dtu.dk/c02807/listings_0-dot-25percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_0-dot-25percent.csv.zip)\n",
    "* 0.5%: [listings](https://data-download.compute.dtu.dk/c02807/listings_0-dot-5percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_0-dot-5percent.csv.zip)\n",
    "* 1%: [listings](https://data-download.compute.dtu.dk/c02807/listings_1-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_1-dot-0percent.csv.zip)\n",
    "* 2%: [listings](https://data-download.compute.dtu.dk/c02807/listings_2-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_2-dot-0percent.csv.zip)\n",
    "* 4%: [listings](https://data-download.compute.dtu.dk/c02807/listings_4-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_4-dot-0percent.csv.zip)\n",
    "* 8%: [listings](https://data-download.compute.dtu.dk/c02807/listings_8-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_8-dot-0percent.csv.zip)\n",
    "* 12.5%: [listings](https://data-download.compute.dtu.dk/c02807/listings_12-dot-5percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_12-dot-5percent.csv.zip)\n",
    "* 16%: [listings](https://data-download.compute.dtu.dk/c02807/listings_16-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_16-dot-0percent.csv.zip)\n",
    "* 25%: [listings](https://data-download.compute.dtu.dk/c02807/listings_25-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_25-dot-0percent.csv.zip)\n",
    "* 50%: [listings](https://data-download.compute.dtu.dk/c02807/listings_50-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_50-dot-0percent.csv.zip)\n",
    "* 75%: [listings](https://data-download.compute.dtu.dk/c02807/listings_75-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_75-dot-0percent.csv.zip)\n",
    "* 100%: [listings](https://data-download.compute.dtu.dk/c02807/listings_100-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_100-dot-0percent.csv.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T23:46:48.667540Z",
     "start_time": "2021-10-28T23:46:48.663425Z"
    },
    "id": "u9BPHTsQ4jnR"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def calculate_word_scores_timed(percent_str):\n",
    "    \"\"\"\n",
    "        Calculates word scores over a sampled dataset indicated\n",
    "        by percent_str.\n",
    "        \n",
    "        :returns: A dictionary with benchmarking information and\n",
    "        the calculated values.\n",
    "    \"\"\"\n",
    "    df_listings = load_csv_as_dataframe(f'listings_{percent_str}percent.csv')\n",
    "    df_reviews = load_csv_as_dataframe(f'reviews_{percent_str}percent.csv')\n",
    "    \n",
    "    listings_count = count_relevant_listings(df_listings, df_reviews)\n",
    "\n",
    "    start = time.time()\n",
    "    df_word_scores = calculate_word_scores(df_listings, df_reviews, listings_count)\n",
    "    end = time.time()\n",
    "    return {\n",
    "        'percentage': float(percent_str.replace('-dot-', '.')), \n",
    "        'time_spent': f\"{end - start:.2f}\", \n",
    "        'relevant_listings': listings_count, \n",
    "        'df': df_word_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.491711Z",
     "start_time": "2021-10-28T23:46:49.516730Z"
    },
    "id": "0eutRq674jnR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-dot-25  done\n",
      "0-dot-5  done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12012/3638621574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpercentage_str\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_percentages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mscore_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpercentage_str\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcalculate_word_scores_timed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpercentage_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpercentage_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12012/170002966.py\u001b[0m in \u001b[0;36mcalculate_word_scores_timed\u001b[1;34m(percent_str)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdf_word_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_word_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_listings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlistings_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     return {\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12012/1900272924.py\u001b[0m in \u001b[0;36mcalculate_word_scores\u001b[1;34m(df_list, df_rev, listings_count)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mdf_top10\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf_sorted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mdf_bottom10\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_sorted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mdf_top10\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;31m# get the bottom 10 words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m#     df_bottom10 = df_occurences.sort(F.asc('word_score')).limit(10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m         \"\"\"\n\u001b[1;32m--> 664\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1038\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1205\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1206\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_percentages = [\n",
    "    '0-dot-25', '0-dot-5', '1-dot-0', '2-dot-0', '4-dot-0', '8-dot-0',\n",
    "    '12-dot-5', '16-dot-0', '25-dot-0'\n",
    "]\n",
    "\n",
    "score_data = {}\n",
    "\n",
    "for percentage_str in data_percentages:\n",
    "    score_data[percentage_str]=calculate_word_scores_timed(percentage_str)\n",
    "    print(percentage_str, ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.502845Z",
     "start_time": "2021-10-29T00:35:02.493477Z"
    },
    "id": "6gn8hhmq4jnR"
   },
   "outputs": [],
   "source": [
    "score_data['50-dot-0'] = calculate_word_scores_timed('50-dot-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.506905Z",
     "start_time": "2021-10-29T00:35:02.503911Z"
    },
    "id": "o6S0UVl74jnS"
   },
   "outputs": [],
   "source": [
    "score_data['75-dot-0'] = calculate_word_scores_timed('75-dot-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.511277Z",
     "start_time": "2021-10-29T00:35:02.510002Z"
    },
    "id": "Co5DP5jH4jnS"
   },
   "outputs": [],
   "source": [
    "score_data['100-dot-0'] = calculate_word_scores_timed('100-dot-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.580856Z",
     "start_time": "2021-10-29T00:35:02.512026Z"
    },
    "id": "KXicfeA-4jnS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scores_scaling = pd.DataFrame(score_data).T.convert_dtypes()\n",
    "df_scores_scaling.time_spent = df_scores_scaling.time_spent.astype(float)\n",
    "\n",
    "# Access to word scores of 2 percent data: df_scores_scaling.loc['2-dot-0'].df\n",
    "df_scores_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.872708Z",
     "start_time": "2021-10-29T00:35:02.582105Z"
    },
    "id": "pEviNX0y4jnS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "lower_range = ['0-dot-25', '0-dot-5', '1-dot-0', '2-dot-0', '4-dot-0', '8-dot-0', '16-dot-0']\n",
    "df_scores_scaling[df_scores_scaling.index.isin(lower_range)] \\\n",
    "    .plot.line(x='percentage', y='time_spent', ax=axes[0], style='-o', title='Lower range')\n",
    "df_scores_scaling[~df_scores_scaling.index.isin(lower_range)] \\\n",
    "    .plot.line(x='percentage', y='time_spent', ax=axes[1], style='-o', title='Upper range')\n",
    "_ = df_scores_scaling \\\n",
    "    .plot.line(x='percentage', y='time_spent', ax=axes[2], style='-o', title='All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf5c4daR4jnS"
   },
   "source": [
    "*Your explanation to the questions outlined at the start of this part goes here. Make sure you've addressed all questions asked.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-61VA-34jnS"
   },
   "source": [
    "## Part 4: Robustness (5 pts)\n",
    "\n",
    "In this part we'll explore robustness of our word scores, using the values we computed in part 3. We'll do so by comparing top/bottom words for three different samples of the dataset. Specifically, the scores from your maximum (e.g. 100%) computed sample are to be compared with the 12.5% and 2.0% scores.\n",
    "\n",
    "Compute and display a dataframe that accounts for any word found in either of the three samples' top/bottom words, and additionally shows the related `word_score` and `word_occurences` values.\n",
    "\n",
    "Note that `df_scores_scaling.loc['100-dot-0'].df` provides the word scores dataframe of the 100% sample (similarly for the other two). For this part you should rely on pandas functionality only.  Moreover, `pd.set_option('display.max_rows', <n>)` should be set with sufficiently high `n` to show all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:37:40.614597Z",
     "start_time": "2021-10-31T14:37:40.612561Z"
    },
    "id": "9jrmvVmM4jnS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQyEFla44jnS"
   },
   "source": [
    "# Exercise 4: Transactions analysis (55 pts)\n",
    "\n",
    "In this exercise the goal is to analyse historical business transactions (sales of parts to other companies), and derive insights about both products and customers.\n",
    "\n",
    "The company X produces and globally sells gadget parts to a number of other companies. You requested the sales department of X to provide you with access to the customer and sales transactions database. To your horror, you've found no such database exists, but the data is instead manually maintained in a spreadsheet (error-prone solution). Intrepid as you are, you've accepted to receive the spreadsheet data as a `.csv`, realizing already data cleaning will be necessary.\n",
    "\n",
    "Your first step (parts 1 and 2) is to clean the data after which you will derive insights about X's business operations (parts 3 and 4).\n",
    "\n",
    "The input data is available here: [transactions.csv](http://courses.compute.dtu.dk/02807/2021/projects/project2/transactions.csv)\n",
    "\n",
    "**Using SQL**\n",
    "\n",
    "In this final exercise you must write SQL to do the data processing in parts 3 and 4. This entails using `psql.sqldf` to execute your queries (up against `df_transactions_cleaned`) which will return a pandas dataframe. Each question should be answered with a *single* query. For visualisation the `psql.sqldf` call should be followed only by functions necessary to customize the plotting/layout steps or reshape the dataframe (i.e. no data processing take place after your SQL statement is materialized as a pandas dataframe).\n",
    "\n",
    "In part 1 and 2 of this exercise, you should make use of pandas functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM2KEdqu4jnS"
   },
   "source": [
    "## Part 1: Data cleaning (15 pts)\n",
    "\n",
    "For each column in the dataframe, investigate and **correct** problematic aspects such as,\n",
    "* Missing values: Insert meaningful values (data imputation). Detectable as `np.nan`'s. A typical value for imputation is the *mode* (most frequent value) of the distribution. If no proper data imputation is possible, you may resort to dropping rows.\n",
    "* Incorrect values: Typos and other data mishaps are present as values are manually entered. Detectable as low-prevalence categorical values, or ambigious data links (e.g. company listed in multiple countries). If no proper value correction is possible, you may resort to dropping rows.\n",
    "\n",
    "In both cases, your strategy for replacing values should be data-driven, that is, shaped by the patterns you observe in the data. It is allowed to skip correcting the data (and instead drop the rows) if few rows are improved by your corrections. If in doubt, do the correction.\n",
    "\n",
    "After all your cleaning steps are completed, you should run the `PandasProfiler` on your cleaned dataset, which should now contain 0% missing cells. Lastly, summarize the issues you identified and how you addressed them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9EAimzb4jnS"
   },
   "source": [
    "### Read, profile and explain\n",
    "\n",
    "As the first step, load the data naming the dataframe `df_transactions`, and make a copy named `df_transactions_cleaned` on which your data cleaning steps will be done. Establish an overview using `PandasProfiler` (but realize there's more to cleaning than what this tool will let you know). Write a paragraph on what the data is about (e.g. what does a row constitute), and a paragraph on what the profile report tells you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:37:53.403596Z",
     "start_time": "2021-10-31T14:37:53.400458Z"
    },
    "id": "A4gokJNI4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KwNFkhZ4jnT"
   },
   "source": [
    "*Your explanation here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uCcxzh54jnT"
   },
   "source": [
    "### Country column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:10.817218Z",
     "start_time": "2021-10-31T14:38:10.814361Z"
    },
    "id": "UQobGiSQ4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnxk7N984jnT"
   },
   "source": [
    "### Company column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:23.615156Z",
     "start_time": "2021-10-31T14:38:23.612981Z"
    },
    "id": "Z1vr1z1V4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzkzURFj4jnT"
   },
   "source": [
    "### City column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:56.402705Z",
     "start_time": "2021-10-27T18:56:56.401612Z"
    },
    "id": "zOQjBe-94jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN671FEX4jnT"
   },
   "source": [
    "### Parts column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:56.644628Z",
     "start_time": "2021-10-27T18:56:56.643615Z"
    },
    "id": "nMHzx5rl4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Stk-AVkM4jnT"
   },
   "source": [
    "### Price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:56.662243Z",
     "start_time": "2021-10-27T18:56:56.660643Z"
    },
    "id": "bGb4G97k4jnU"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9TgwrNZ4jnU"
   },
   "source": [
    "### Date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:43.270635Z",
     "start_time": "2021-10-31T14:38:43.268585Z"
    },
    "id": "am9CNN594jnU"
   },
   "outputs": [],
   "source": [
    "#your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ik77rvq4jnU"
   },
   "source": [
    "### Profile `df_transactions_cleaned` and summarize corrections made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:52.078805Z",
     "start_time": "2021-10-31T14:38:52.076381Z"
    },
    "id": "vOuZ6QGY4jnU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTnK1fMF4jnU"
   },
   "source": [
    "*Your summary goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVEOeN4C4jnU"
   },
   "source": [
    "## Part 2: Standardise prices (5 pts)\n",
    "\n",
    "Transaction prices are recorded in the local currency of the client (EUR, GBP, USD or JPY). You will need to convert these prices from local currency into the common currency (chosen here as) EUR, for comparability. These standardised prices should be added as a column to the dataframe called `prices_euro`.\n",
    "\n",
    "Consider a two step process where you 1) Identify what currency has been used, and 2) Calculate the price conversion. Step 1 may reveal the data is still not completely clean (so either correct by impute or drop). For Step 2 look up exchange rates on the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:58.558049Z",
     "start_time": "2021-10-27T18:56:58.556556Z"
    },
    "id": "VTFTFPMX4jnU"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Yiqzq3L4jnU"
   },
   "source": [
    "## Part 3: Business insights (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L019f5Sb4jnV"
   },
   "source": [
    "### Company by revenue\n",
    "\n",
    "The revenue of a company is its total value of orders, all time. Compute and visualise all companies by revenue in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:13.954452Z",
     "start_time": "2021-10-31T14:39:13.952349Z"
    },
    "id": "qS67VJG-4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VL29OwOV4jnV"
   },
   "source": [
    "### Country by revenue, per year\n",
    "\n",
    "The revenue of a country in a time period, is its total value of orders in that time period. Compute and visualise all countries by revenue, for years 2016, 2017 and 2018. Your visualisation should have countries on the x-axis and multiple bars (one for each year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:18.597583Z",
     "start_time": "2021-10-31T14:39:18.594419Z"
    },
    "id": "_KHGZs0O4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIwka6bY4jnV"
   },
   "source": [
    "### Orders per quarter, all companies\n",
    "\n",
    "Compute and visualise the number of orders each company has placed in each quarter. Exclude quarters where the order count is less than 3. As always, be mindful to not produce a cluttered visualisation.\n",
    "\n",
    "Part of your query should form a variable that converts `date` into `YEAR_QUARTER` format. Dealing with dates is via `STRFTIME` [docs](https://www.sqlite.org/lang_datefunc.html) which doesn't allow quarter extraction. Instead, it allows for extraction of month, which you can case on in order to produce the quarter (Q1, Q2, Q3, Q4).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:23.632721Z",
     "start_time": "2021-10-31T14:39:23.629720Z"
    },
    "id": "zGT4oqVl4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW0LZ7_b4jnV"
   },
   "source": [
    "## Part 4: Parts and prices (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM6CcJQj4jnV"
   },
   "source": [
    "### Parts demand changes\n",
    "\n",
    "A different amount of orders are placed on parts each year. The demand of a part is the number of orders placed on it. The demand change of a part is the absolute difference between its average demand in 2016/2017, and its demand in 2018.\n",
    "\n",
    "Compute and visualise the 15 parts whose demand change has been the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:30.418442Z",
     "start_time": "2021-10-31T14:39:30.415270Z"
    },
    "id": "qjh6pszu4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNGVkZIL4jnV"
   },
   "source": [
    "### Popular parts pricing\n",
    "\n",
    "The most popular parts are those whose demand has increased the most from its 2016/2017 average to 2018. We're interested to find out if popularity is due to a price drop, and also inform us if prices of these parts are properly adjusted.\n",
    "\n",
    "The demand increase of a part is its 2018 demand minus its 2016/2017 average demand. The price change of a part is its average 2018 price minus its average 2016/2017 price.\n",
    "\n",
    "Compute the parts whose demand has increased (has positive demand increase) and the change in price for each of these parts. Then visualise this relationship and include in the figure title the correlation (compute via pandas) between these two variables. Conclude which is most likely 1) Parts became more popular from a drop in prices, or 2) The sales department deserved its bonuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:38.813945Z",
     "start_time": "2021-10-31T14:39:38.811979Z"
    },
    "id": "prx6EWk04jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTYM02c14jnW"
   },
   "source": [
    "*your explanation goes here*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02807_Project_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "432.448px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
